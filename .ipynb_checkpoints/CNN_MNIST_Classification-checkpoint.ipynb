{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75bbd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d34e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify mean and Stander devition of all pixels in Mnist dataset --> precomputed\n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "# Transform the image to tenser\n",
    "# Normalize the tenser using Mean and Stander_deviation\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((mean_gray),(stddev_gray))])\n",
    "\n",
    "# loading The dataset\n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                              train=True,\n",
    "                              transform=transform,\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                              train=False,\n",
    "                              transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fda424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x213a4a81a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visulize training dataset\n",
    "import matplotlib.pyplot as plt\n",
    "random_image = train_dataset[20][0].numpy() * stddev_gray + mean_gray\n",
    "plt.imshow(random_image.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a12803-e6ca-4d7b-896c-59d1e95a2cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[20][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab912c2-3fbb-4af6-992b-d51c1090eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "batch_size =100\n",
    "train_load = torch.utils.data.DataLoader(dataset = train_dataset \n",
    "                                        ,batch_size=batch_size,\n",
    "                                        shuffle = True)\n",
    "test_load = torch.utils.data.DataLoader(dataset = test_dataset \n",
    "                                        ,batch_size=batch_size,\n",
    "                                        shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e81b7a27-9614-4102-bcf3-8452e5b7490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "--------------\n",
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_dataset))\n",
    "len(test_load)\n",
    "print(\"--------------\")\n",
    "print(len(train_dataset))\n",
    "len(train_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb538ae5-7516-4ca4-a3bf-41a77f9be81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #same padding --> input_size = output_size \n",
    "        # input size =28\n",
    "        #same padding =(Kernal_size-1)/2  (3-1)/2 = 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1 , out_channels=8 ,kernel_size=3 ,stride=1 ,padding=1)\n",
    "        # output size of 8 feature map = ((input_size-filter_size +(2*padding))/stride) + 1\n",
    "        #out_size = ((28 - 3 +(2*1))/1)+1 =28  \n",
    "        self.batchnorm1 = nn.BatchNorm2d(8) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool= nn.MaxPool2d(kernel_size=2)\n",
    "        #out_size =28/2 =14\n",
    "        \n",
    "        \n",
    "        #same padding (kernal_size -1)/2 ---> (5-1)/2= 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1,padding=2)\n",
    "        #out_size = ((input_size-filter_size +(2*padding))/stride) + 1\n",
    "        #out_size= (14-5+(2*2)/1)  +1 -->14\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        # after apllying maxpooling again the output size = 14/2 = 7\n",
    "        \n",
    "        #befer aplly fully conected layer we will flatten the image\n",
    "        #Flatten the 32 feature map --> 32*7*7 = 1568\n",
    "        self.fc1 = nn.Linear(1568,600)\n",
    "        self.dropout = nn.Dropout(p=0.5) # apply dropout\n",
    "        self.fc2 = nn.Linear(600,10) # we have 10 clasees\n",
    "        \n",
    "    def forward(self, x): \n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        #Flatten the 32 feature map from max pooling 2 to feed it to fc1 -- 32*7*7 = 1568\n",
    "        out = out.view(-1,1568)\n",
    "        #Then feed forward\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c85810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0262a",
   "metadata": {},
   "source": [
    "## Understanding the propgation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae024267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for one iteration this what happen \n",
      "Input shape torch.Size([100, 1, 28, 28])\n",
      "labels shape torch.Size([100])\n",
      "output shape is torch.Size([100, 10])\n",
      "predicted shape is torch.Size([100])\n",
      "predicted tensor: \n",
      " tensor([7, 2, 2, 3, 7, 7, 7, 2, 3, 2, 3, 7, 7, 3, 7, 3, 7, 7, 7, 7, 3, 3, 9, 7,\n",
      "        3, 6, 7, 7, 3, 5, 2, 3, 1, 3, 7, 7, 3, 2, 6, 7, 7, 3, 3, 6, 3, 2, 2, 7,\n",
      "        2, 9, 6, 3, 4, 5, 9, 2, 3, 4, 7, 7, 9, 7, 3, 7, 3, 7, 5, 3, 7, 7, 5, 3,\n",
      "        7, 3, 7, 7, 7, 7, 7, 7, 3, 2, 8, 5, 1, 7, 7, 7, 6, 3, 7, 3, 5, 9, 7, 4,\n",
      "        7, 5, 7, 3])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "iteration = 0 \n",
    "correct = 0\n",
    "for i, (inputs, labels) in enumerate (train_load):\n",
    "    \n",
    "    print('for one iteration this what happen ')\n",
    "    #Each tenser prooagated through the network should be 4d\n",
    "    #(batch_size, channel, rows ,columns)\n",
    "    \n",
    "    print('Input shape',inputs.shape)\n",
    "    print('labels shape',labels.shape)\n",
    "    \n",
    "    output = model(inputs) # Aplly forward propagation\n",
    "    print(\"output shape is\",output.shape)\n",
    "    \n",
    "    _, predicted = torch.max(output,1)\n",
    "    print(\"predicted shape is\",predicted.shape)\n",
    "    print(\"predicted tensor: \\n\",predicted)\n",
    "    print(len(predicted))\n",
    "    correct += (predicted == labels).sum()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fd4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training The Cnn\n",
    "num_epochs = 10\n",
    "train_loss = []\n",
    "train_accurcy = []\n",
    "test_loss = []\n",
    "test_accurcy =[]\n",
    "\n",
    "#loop for train whlole data\n",
    "for epoch in range (num_epochs):\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0 # loss for each itertion\n",
    "    \n",
    "    #Train mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs ,labels) in enumerate (train_load):\n",
    "        \n",
    "        outputs = model(inputs)  #Aplly forward propgation\n",
    "        loss = loss_fn(outputs, labels) # clculate loss for this iteration\n",
    "        iter_loss += loss.item() # apeend loss for each itertion to all4\n",
    "        \n",
    "        optimizer.zero_grad()# w--> w -lr* gradient\n",
    "        loss.backward  # aplly back propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs,1) # store index of high predicted\n",
    "        correct += (predicted==labels)\n",
    "        iterations += 1\n",
    "        \n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    train_accurcy.append(correct/len(train_load))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072ab2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
